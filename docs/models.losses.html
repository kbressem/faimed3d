---

title: Losses and metrics


keywords: fastai
sidebar: home_sidebar



nb_path: "nbs/05d_models.losses.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/05d_models.losses.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Loss-functions">Loss functions<a class="anchor-link" href="#Loss-functions"> </a></h2><h3 id="DICE-Loss">DICE Loss<a class="anchor-link" href="#DICE-Loss"> </a></h3>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="DiceLossBinary" class="doc_header"><code>class</code> <code>DiceLossBinary</code><a href="https://github.com/kbressem/faimed3d/tree/master/faimed3d/models/losses.py#L13" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>DiceLossBinary</code>(<strong><code>method</code></strong>=<em><code>'miletari'</code></em>, <strong><code>alpha</code></strong>=<em><code>0.5</code></em>, <strong><code>beta</code></strong>=<em><code>0.5</code></em>, <strong><code>eps</code></strong>=<em><code>1e-07</code></em>, <strong><code>smooth</code></strong>=<em><code>1.0</code></em>)</p>
</blockquote>
<p>Simple DICE loss as described in:
    <a href="https://arxiv.org/pdf/1911.02855.pdf">https://arxiv.org/pdf/1911.02855.pdf</a></p>
<p>Computes the Sørensen–Dice loss. Larger is better.
Note that PyTorch optimizers minimize a loss. So the loss is subtracted from 1.</p>
<p>Args:
    targ:    A tensor of shape [B, 1, D, H, W].
    pred:    A tensor of shape [B, 1, D, H, W]. Corresponds to
             the raw output or logits of the model.
    method:  The method, how the DICE score should be calcualted.
                "simple"   = standard DICE loss
                "miletari" = squared denominator for faster convergence
                "tversky"  = variant of the DICE loss which allows to weight FP vs FN.
    alpha, beta: weights for FP and FN for "tversky" loss, if both values are 0.5 the
             "tversky" loss corresponds to the "simple" DICE loss
    smooth:  Added smoothing factor.
    eps: added to the denominator for numerical stability (acoid division by 0).
Returns:
    dice_loss: the Sørensen–Dice loss.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="DiceLossMulti" class="doc_header"><code>class</code> <code>DiceLossMulti</code><a href="https://github.com/kbressem/faimed3d/tree/master/faimed3d/models/losses.py#L71" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>DiceLossMulti</code>(<strong><code>n_classes</code></strong>, <strong><code>weights</code></strong>=<em><code>None</code></em>, <strong>**<code>kwargs</code></strong>) :: <a href="/faimed3d/models.losses.html#DiceLossBinary"><code>DiceLossBinary</code></a></p>
</blockquote>
<p>Simple DICE loss as described in:
    <a href="https://arxiv.org/pdf/1911.02855.pdf">https://arxiv.org/pdf/1911.02855.pdf</a></p>
<p>Computes the Sørensen–Dice loss. Larger is better.
Note that PyTorch optimizers minimize a loss. So the loss is subtracted from 1.</p>
<p>Args:
    targ:    A tensor of shape [B, 1, D, H, W].
    pred:    A tensor of shape [B, 1, D, H, W]. Corresponds to
             the raw output or logits of the model.
    method:  The method, how the DICE score should be calcualted.
                "simple"   = standard DICE loss
                "miletari" = squared denominator for faster convergence
                "tversky"  = variant of the DICE loss which allows to weight FP vs FN.
    alpha, beta: weights for FP and FN for "tversky" loss, if both values are 0.5 the
             "tversky" loss corresponds to the "simple" DICE loss
    smooth:  Added smoothing factor.
    eps: added to the denominator for numerical stability (acoid division by 0).
Returns:
    dice_loss: the Sørensen–Dice loss.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="MCC-Loss">MCC Loss<a class="anchor-link" href="#MCC-Loss"> </a></h3><p>Implementing the MCC score as loss function:</p>
$$\frac{ 
        \sum_{i}^{n} p_{ i }g_{ i } * \sum_{i}^{n}  1-p_{ i } 1-g_{ i } +
        \sum_{i}^{n}  1-p_{ i } g_{ i } * \sum_{i}^{n}  p_{ i } 1-g_{ i }}{ \sqrt{ 
        (\sum_{i}^{n}  p_{ i } g_{ i } + \sum_{i}^{n}  1-p_{ i } g_{ i }) * 
        (\sum_{i}^{n}  p_{ i } g_{ i } + \sum_{i}^{n} p_{ i } 1-g_{ i }) *  
        (\sum_{i}^{n}  1-p_{ i } g_{ i } + \sum_{i}^{n} 1-p_{ i } 1-g_{ i }) * 
        (\sum_{i}^{n}  p_{ i } 1-g_{ i } + \sum_{i}^{n} 1-p_{ i } 1-g_{ i }) 
     } }$$<p>where p_i is the prediction for pixel i and g_i the corresponding ground truth pixel and gamma is the smoothing factor.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="MCCLossBinary" class="doc_header"><code>class</code> <code>MCCLossBinary</code><a href="https://github.com/kbressem/faimed3d/tree/master/faimed3d/models/losses.py#L111" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>MCCLossBinary</code>(<strong>**<code>kwargs</code></strong>) :: <a href="/faimed3d/models.losses.html#DiceLossBinary"><code>DiceLossBinary</code></a></p>
</blockquote>
<p>Computes the MCC loss.</p>
<p>From Wikipedia (<a href="https://en.wikipedia.org/wiki/Matthews_correlation_coefficient">https://en.wikipedia.org/wiki/Matthews_correlation_coefficient</a>):</p>

<pre><code>&gt; The coefficient takes into account true and false positives and negatives and is generally
&gt; regarded as a balanced measure which can be used even if the classes are of very different sizes
&gt; The MCC is in essence a correlation coefficient between the observed
&gt; and predicted binary classifications; it returns a value between −1 and +1.
&gt; A coefficient of +1 represents a perfect prediction, 0 no better than random prediction
&gt; and −1 indicates total disagreement between prediction and observation

</code></pre>
<p>For this loss to work best, the input should be in range 0-1, e.g. enforced through a sigmoid or softmax.
Note that PyTorch optimizers minimize a loss. So the loss is subtracted from 1.</p>
<p>Math:
    rac{
        \sum<em>{i}^{n} p</em>{ i }g<em>{ i } * \sum</em>{i}^{n}  1-p<em>{ i } 1-g</em>{ i } +
        \sum<em>{i}^{n}  1-p</em>{ i } g<em>{ i } * \sum</em>{i}^{n}  p<em>{ i } 1-g</em>{ i }}{ \sqrt{
        (\sum<em>{i}^{n}  p</em>{ i } g<em>{ i } + \sum</em>{i}^{n}  1-p<em>{ i } g</em>{ i }) <em>
        (\sum<em>{i}^{n}  p</em>{ i } g<em>{ i } + \sum</em>{i}^{n} p<em>{ i } 1-g</em>{ i }) </em>
        (\sum<em>{i}^{n}  1-p</em>{ i } g<em>{ i } + \sum</em>{i}^{n} 1-p<em>{ i } 1-g</em>{ i }) *
        (\sum<em>{i}^{n}  p</em>{ i } 1-g<em>{ i } + \sum</em>{i}^{n} 1-p<em>{ i } 1-g</em>{ i })
     } }</p>
<p>Args:
    input:   A tensor of shape [B, 1, D, H, W]. Predictions.
    target:  A tensor of shape [B, 1, D, H, W]. Ground truth.
    smooth:  Smoothing factor, default is 1. Inherited from DiceLossBinary base class
    eps:     Added for numerical stability.
Returns:
    mmc_loss: loss based on Matthews correlation coefficient</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">i</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">25</span><span class="p">,</span><span class="mi">25</span><span class="p">)</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">()</span>
<span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">25</span><span class="p">,</span><span class="mi">25</span><span class="p">)</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">()</span><span class="o">.</span><span class="n">round</span><span class="p">()</span>

<span class="n">MCCLossBinary</span><span class="p">()(</span><span class="n">i</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([1.0011])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="MCCLossMulti" class="doc_header"><code>class</code> <code>MCCLossMulti</code><a href="https://github.com/kbressem/faimed3d/tree/master/faimed3d/models/losses.py#L171" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>MCCLossMulti</code>(<strong><code>n_classes</code></strong>, <strong><code>weights</code></strong>=<em><code>None</code></em>, <strong>**<code>kwargs</code></strong>) :: <a href="/faimed3d/models.losses.html#MCCLossBinary"><code>MCCLossBinary</code></a></p>
</blockquote>
<p>Computes the MCC loss for a multilabel target. Basically the same as <a href="/faimed3d/models.losses.html#MCCLossBinary"><code>MCCLossBinary</code></a>
but one hot encodes the target before computation.</p>
<p>Args:
    num_features: Number of different features in y.
             Must correspond to the maximum number of overall features in the whole dataset.
    input:   A tensor of shape [B, C, D, H, W], where the <code>n_classes</code> should correspond to C.
    target:  A tensor of shape [B, 1, D, H, W] or [B, C, D, H, W] where C is the same size as in the input.
    weights: Either a str: 'auto' for autocalculation, None or a list/tuple of soecified weights
    smooth:  Smoothing factor, default is 1. Inherited from DiceLossBinary base class
    eps:     Added for numerical stability.
    n_classes: number of classes to predict
Returns:
    mcc_loss: loss based on Matthews correlation coefficient</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">i</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">25</span><span class="p">,</span><span class="mi">25</span><span class="p">)</span>
<span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">25</span><span class="p">,</span><span class="mi">25</span><span class="p">))</span>

<span class="n">MCCLossMulti</span><span class="p">(</span><span class="mi">5</span><span class="p">)(</span><span class="n">i</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor(1.0020)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="SoftMCCLossMulti" class="doc_header"><code>class</code> <code>SoftMCCLossMulti</code><a href="https://github.com/kbressem/faimed3d/tree/master/faimed3d/models/losses.py#L228" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>SoftMCCLossMulti</code>(<strong><code>n_classes</code></strong>, <strong><code>weights</code></strong>=<em><code>None</code></em>, <strong>**<code>kwargs</code></strong>) :: <a href="/faimed3d/models.losses.html#MCCLossMulti"><code>MCCLossMulti</code></a></p>
</blockquote>
<p>Same as MCCLossMulti but can handle float values.
Example:
    t = torch.randn(2,5); t</p>

<pre><code>&gt;&gt;&gt; tensor([[ 0.9113, -0.7525, -2.1771, -0.2420, -0.2245],
            [ 1.9503, -1.2903,  0.1201,  0.2830,  0.0473]])

MCCLossMulti(2).make_binary(t, 1)
&gt;&gt;&gt; tensor([[0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0.]])

SoftMCCLossMulti(2).soft_binary(t, 0)
&gt;&gt;&gt; tensor([[0.9113, 0.0000, 0.0000, 0.0000, 0.0000],
            [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]])</code></pre>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">SoftMCCLossMulti</span><span class="p">(</span><span class="mi">5</span><span class="p">)(</span><span class="n">i</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor(1.0059)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="WeightedMCCLossMulti" class="doc_header"><code>class</code> <code>WeightedMCCLossMulti</code><a href="https://github.com/kbressem/faimed3d/tree/master/faimed3d/models/losses.py#L256" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>WeightedMCCLossMulti</code>(<strong><code>gamma</code></strong>=<em><code>0.5</code></em>, <strong><code>delta</code></strong>=<em><code>0.5</code></em>, <strong>*<code>args</code></strong>, <strong>**<code>kwargs</code></strong>) :: <a href="/faimed3d/models.losses.html#MCCLossMulti"><code>MCCLossMulti</code></a></p>
</blockquote>
<p>Weighted version of <a href="/faimed3d/models.losses.html#MCCLossMulti"><code>MCCLossMulti</code></a>.
Note that class specific weight can still be added through <code>weights</code> during initialization.</p>
<p>Args:
    alpha: weight for true positives
    beta: weight for false positives
    gamma: weight for false negatives
    delta: weight for true negatives</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Metrics">Metrics<a class="anchor-link" href="#Metrics"> </a></h1>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="MCCScore" class="doc_header"><code>class</code> <code>MCCScore</code><a href="https://github.com/kbressem/faimed3d/tree/master/faimed3d/models/losses.py#L293" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>MCCScore</code>(<strong><code>n_classes</code></strong>=<em><code>None</code></em>, <strong><code>thres</code></strong>=<em><code>0.5</code></em>, <strong>**<code>kwargs</code></strong>) :: <a href="/faimed3d/models.losses.html#MCCLossMulti"><code>MCCLossMulti</code></a></p>
</blockquote>
<p>Computes the MCC loss for a multilabel target. Basically the same as <a href="/faimed3d/models.losses.html#MCCLossBinary"><code>MCCLossBinary</code></a>
but one hot encodes the target before computation.</p>
<p>Args:
    num_features: Number of different features in y.
             Must correspond to the maximum number of overall features in the whole dataset.
    input:   A tensor of shape [B, C, D, H, W], where the <code>n_classes</code> should correspond to C.
    target:  A tensor of shape [B, 1, D, H, W] or [B, C, D, H, W] where C is the same size as in the input.
    weights: Either a str: 'auto' for autocalculation, None or a list/tuple of soecified weights
    smooth:  Smoothing factor, default is 1. Inherited from DiceLossBinary base class
    eps:     Added for numerical stability.
    n_classes: number of classes to predict
Returns:
    mcc_loss: loss based on Matthews correlation coefficient</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">MCCScore</span><span class="p">()(</span><span class="n">i</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor(0.0062)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

</div>
 

