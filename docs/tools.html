---

title: Task specific functions


keywords: fastai
sidebar: home_sidebar



nb_path: "nbs/99_tools.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/99_tools.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">fastai.vision.all</span> <span class="kn">import</span> <span class="o">*</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Flipping-NIfTI-images">Flipping NIfTI images<a class="anchor-link" href="#Flipping-NIfTI-images"> </a></h2><p>When displaying the masks it may occure, that Nifti images are rotated and do not fit the original DICOM. Most likely it is due to different frames of reference. 
in Simple-ITK it is LPS, while NIfTI and FSL use RAS, so those matrices are the same after accounting for frame of reference (taken and adapted from <a href="https://discourse.itk.org/t/nifti-orientation-issues/431">https://discourse.itk.org/t/nifti-orientation-issues/431</a>).
So, luckly the malrotations are systematic and can be scripted.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="RotateNifti" class="doc_header"><code>class</code> <code>RotateNifti</code><a href="https://github.com/kbressem/faimed3d/tree/master/faimed3d/utils.py#L16" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>RotateNifti</code>()</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">r</span> <span class="o">=</span> <span class="n">RotateNifti</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>/home/bressekk/anaconda3/envs/fastai-v2/lib/python3.7/site-packages/ipykernel_launcher.py:4: UserWarning: This class will change the source files on you disk. Be carefull.
  after removing the cwd from sys.path.
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}
r.rotate_single('../../dl-prostate-mapping/data/train/ProstataCa/V0071222489/ADC/Annotation/Untitled.nii.gz')
r.rotate_single('../../dl-prostate-mapping/data/train/ProstataCa/V0071222489/ADC/Annotation/cropped_mask.nii.gz')
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Exporting-subvolumes">Exporting subvolumes<a class="anchor-link" href="#Exporting-subvolumes"> </a></h2><p>In processing medical 3D data, a challenge is to that the regions of interest are often very small. For example, if the goal is to segment and classify kideny cancer from abdominal CT scans, the cancer corresponds to less then 1% of the voxels in the volume. The most voxels are not relevant for the task. One possibility to deal with this problem, is to first train a model which segments the whole kidney and to only export this subvolume. Then a second model can be trained on the subvolumes.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>To convieniently perform predictions, cropping and exporting a <a href="/faimed3d/tools.html#SubvolumeExporter"><code>SubvolumeExporter</code></a> class is created.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="SubvolumeExporter" class="doc_header"><code>class</code> <code>SubvolumeExporter</code><a href="https://github.com/kbressem/faimed3d/tree/master/faimed3d/utils.py#L40" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>SubvolumeExporter</code>()</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>A model needs to be assigned to the SubvolumeExporter, so predictions on new data can be made.</p>

</div>
</div>
</div>SubvolumeExport = SubvolumeExporter() 
SubvolumeExport.assign_model(learn)
SubvolumeExport.assign_tfms(RandomCrop3D(crop_by = (0.2, 0.15, 0.15), rand_crop_xyz = (0,0,0), perc_crop = True))
SubvolumeExport.predict(d.t2_dcm_path[0]) # predict mask on one image
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>To check the quality of the class and predictions, botch can be visualized.</p>

</div>
</div>
</div>SubvolumeExport.show_pair(nrow = 16, figsize = (35, 25))
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Using strip_paor reduces the image size, so that it narrowly fits the mask. 
Per default padding will be 0 so the largest mask diameter will decide on the size of the image. Using pad_z, pad_x, pad_y (or pad_xy instead of pad_x, pad_y) a small marign can be added in case the classes do not fit perfectly. Padding image size larger then original size is not possible and will not lead to black spaces.</p>

</div>
</div>
</div>SubvolumeExport.strip_pair(pad_xy = 5, pad_z = 10)
SubvolumeExport.show_pair(nrow = 16, figsize = (35, 25))
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Reduce-size-of-original-image-and-mask">Reduce size of original image and mask<a class="anchor-link" href="#Reduce-size-of-original-image-and-mask"> </a></h2><p>The predicted mask and theroff derived images can be used for segementation pipelines, however the mask shows only as much imformation as the model did predict. To further train the images, it makes no sense exporting the predicted mask, but the original mask and images need to be cropped and exported. A subclass of <a href="/faimed3d/tools.html#SubvolumeExporter"><code>SubvolumeExporter</code></a> can convert the original files, but the export functions need to be adapted.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="CropOriginalToMask" class="doc_header"><code>class</code> <code>CropOriginalToMask</code><a href="https://github.com/kbressem/faimed3d/tree/master/faimed3d/utils.py#L129" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>CropOriginalToMask</code>() :: <a href="/faimed3d/tools.html#SubvolumeExporter"><code>SubvolumeExporter</code></a></p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}
Crop = CropOriginalToMask()Crop.convert_and_export(d.t2_dcm_path[0], d.t2_mask_base[0])
</div>
 

