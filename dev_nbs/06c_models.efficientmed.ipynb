{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EfficientMed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to optimize network for medical data. Network should have skip connections and self attention (a lot of self attention), so that it can focus on small cancer lesions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class MedConvBlock(nn.Module):\n",
    "    \"\"\"Adapted Mobile Inverted Residual Bottleneck Block\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 n_inp, # number of input channels\n",
    "                 n_out, # number of output channels\n",
    "                 kernel_size, # size of convolution kernel\n",
    "                 stride, # stride of kernel\n",
    "                 se_ratio, # squeeze-expand ratio\n",
    "                 id_skip, # if skip connection shouldbe used\n",
    "                 expand_ratio, # expansion ratio for inverted bottleneck\n",
    "                 drop_connect_rate = 0.2, # percentage of dropped connections\n",
    "                 act_cls=nn.SiLU, # type of activation function\n",
    "                 norm_type=NormType.Batch, # type of batch normalization\n",
    "                 **kwargs # further arguments passed to `ConvLayerDynamicPadding`\n",
    "                ):\n",
    "        super().__init__()\n",
    "        store_attr()\n",
    "        \n",
    "        # expansion phase (inverted bottleneck)\n",
    "        n_intermed = n_inp * expand_ratio  # number of output channels\n",
    "        if expand_ratio != 1: \n",
    "            self.expand_conv = ConvLayer(ni=n_inp, nf=n_intermed, \n",
    "                                                       ks = 1,norm_type=norm_type, \n",
    "                                                       act_cls=act_cls, **kwargs)\n",
    "            \n",
    "        # depthwise convolution phase, groups makes it depthwise\n",
    "        self.depthwise_conv = ConvLayer(ni=n_intermed, nf=n_intermed, \n",
    "                                                      groups=n_intermed, ks=kernel_size, \n",
    "                                                      stride=stride, norm_type=norm_type, \n",
    "                                                      act_cls=act_cls, **kwargs)\n",
    "\n",
    "        # squeeze and excitation layer, if desired\n",
    "        self.has_se = (se_ratio is not None) and (0 < se_ratio <= 1)\n",
    "        if self.has_se:\n",
    "            num_squeezed_channels = max(1, int(n_inp * se_ratio))\n",
    "            self.squeeze_expand = nn.Sequential(\n",
    "                ConvLayer(ni=n_intermed, nf=num_squeezed_channels, ks=1, \n",
    "                                        act_cls=act_cls, norm_type=None, **kwargs), \n",
    "                ConvLayer(ni=num_squeezed_channels, nf=n_intermed, ks=1,  \n",
    "                                        act_cls=None, norm_type=None,**kwargs))\n",
    "\n",
    "        # pointwise convolution phase\n",
    "        self.project_conv = ConvLayerDynamicPadding(ni=n_intermed, nf=n_out, ks=1, \n",
    "                                                    act_cls = None, **kwargs)\n",
    "        self.drop_conncet = DropConnect(drop_connect_rate)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.id_skip: inputs = x # save input only if skip connection \n",
    "        \n",
    "        # expansion \n",
    "        if self.expand_ratio != 1: x = self.expand_conv(x)\n",
    "        \n",
    "        # depthwise convolution\n",
    "        x = self.depthwise_conv(x)\n",
    "\n",
    "        # squeeze and excitation (self attention)\n",
    "        if self.has_se:\n",
    "            x_squeezed = F.adaptive_avg_pool3d(x, 1)\n",
    "            x_squeezed = self.squeeze_expand(x_squeezed)\n",
    "            x = x * x_squeezed.sigmoid() # inplace saves a bit of memory\n",
    "    \n",
    "        # pointwise convolution\n",
    "        x = self.project_conv(x)\n",
    "\n",
    "        # skip connection and drop connect\n",
    "        if self.id_skip and self.stride == 1 and self.n_inp == self.n_out:\n",
    "            x = self.drop_conncet(x) + inputs  # skip connection\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.layers import *\n",
    "from fastai.basics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SEBlock(Module):\n",
    "    def __init__(self):\n",
    "        self.pooling = nn.AdaptiveAvgPool3d(1)\n",
    "        self.conv1 = nn.Conv3d(80, 10, kernel_size=1)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv3d(10, 80, kernel_size=1)\n",
    "        self.act2 = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_squeezed = self.pooling(x)\n",
    "        x_squeezed = self.conv1(x_squeezed)\n",
    "        x_squeezed = self.act1(x_squeezed)\n",
    "        x_squeezed = self.conv2(x_squeezed)\n",
    "        x_squeezed = self.act2(x_squeezed)\n",
    "        x = x * x_squeezed\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 80, 10, 50, 50])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEBlock()(torch.randn(1, 80, 10, 50, 50)).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squeeze_expand = nn.Sequential(\n",
    "                ConvLayer(ni=n_intermed, nf=num_squeezed_channels, ks=1, \n",
    "                                        act_cls=act_cls, norm_type=None, **kwargs), \n",
    "                ConvLayer(ni=num_squeezed_channels, nf=n_intermed, ks=1,  \n",
    "                                        act_cls=None, norm_type=None,**kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 01_basics.ipynb.\n",
      "Converted 02_preprocessing.ipynb.\n",
      "Converted 03_transforms.ipynb.\n",
      "Converted 04_dataloaders.ipynb.\n",
      "Converted 05_learner.ipynb.\n",
      "Converted 06a_models.alexnet.ipynb.\n",
      "Converted 06b_models.resnet.ipynb.\n",
      "Converted 06c_models.densenet.ipynb.\n",
      "Converted 06d_models.DynamicUnet.ipynb.\n",
      "Converted 06d_models.unet.ipynb.\n",
      "Converted 06e_models.deeplabv3.ipynb.\n",
      "Converted 06f_models.losses.ipynb.\n",
      "Converted 07_callback.ipynb.\n",
      "Converted 99_tools.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai",
   "language": "python",
   "name": "fastai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
