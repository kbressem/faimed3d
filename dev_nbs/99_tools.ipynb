{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task specific functions\n",
    "\n",
    "The classes and functions in this notebook are highly specific and probably not usefull for the most tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# default_exp utils\n",
    "\n",
    "from fastai.basics import *\n",
    "import pathlib\n",
    "import SimpleITK as sitk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from faimed3d.all import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flipping NIfTI images\n",
    "When displaying the masks it may occure, that Nifti images are rotated and do not fit the original DICOM. Most likely it is due to different frames of reference. \n",
    "in Simple-ITK it is LPS, while NIfTI and FSL use RAS, so those matrices are the same after accounting for frame of reference (taken and adapted from https://discourse.itk.org/t/nifti-orientation-issues/431).\n",
    "So, luckly the malrotations are systematic and can be scripted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export utils\n",
    "class RotateNifti():\n",
    "    def __init__(self):\n",
    "        warn('This class will change the source files on you disk. Be carefull.')\n",
    "    \n",
    "    def rotate_single(self, fn:(Path, str)):\n",
    "        if isinstance(fn, Path): fn = str(fn) \n",
    "        \n",
    "        im = sitk.ReadImage(fn)\n",
    "        arr = sitk.GetArrayFromImage(im)\n",
    "        arr = np.rot90(arr, 0)\n",
    "        arr = np.flip(arr, 1)\n",
    "        im2 = sitk.GetImageFromArray(arr)\n",
    "        for k in im.GetMetaDataKeys(): # Copy meta data from original image before overwriting it. \n",
    "            im2.SetMetaData(k, im.GetMetaData(k))\n",
    "        sitk.WriteImage(im2, fn)\n",
    "   \n",
    "    def rotate_list(self, file_list=None, verbose = True):\n",
    "        if file_list is not None: self.file_list = file_list\n",
    "    \n",
    "        for fn in file_list: \n",
    "            self.rotate_single(fn)\n",
    "            if verbose: print('converted file at: '+str(fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bressekk/anaconda3/envs/fastai-v2/lib/python3.7/site-packages/ipykernel_launcher.py:4: UserWarning: This class will change the source files on you disk. Be carefull.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "r = RotateNifti()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "r.rotate_single('../../dl-prostate-mapping/data/train/ProstataCa/V0071222489/ADC/Annotation/Untitled.nii.gz')\n",
    "r.rotate_single('../../dl-prostate-mapping/data/train/ProstataCa/V0071222489/ADC/Annotation/cropped_mask.nii.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting subvolumes\n",
    "\n",
    "In processing medical 3D data, a challenge is to that the regions of interest are often very small. For example, if the goal is to segment and classify kideny cancer from abdominal CT scans, the cancer corresponds to less then 1% of the voxels in the volume. The most voxels are not relevant for the task. One possibility to deal with this problem, is to first train a model which segments the whole kidney and to only export this subvolume. Then a second model can be trained on the subvolumes. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convieniently perform predictions, cropping and exporting a `SubvolumeExporter` class is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class SubvolumeExporter(object):\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.item_tfms = None\n",
    "    \n",
    "    def assign_model(self, model):\n",
    "        self.model = model\n",
    "        \n",
    "    def assign_tfms(self, tfms):\n",
    "        \"assign transformations which should be applied to the surce, before size of source and mask are matched\"\n",
    "        self.item_tfms = tfms\n",
    "    \n",
    "    def merge_mask(self, mask):\n",
    "        mask = mask.squeeze()\n",
    "        if mask.ndim == 4:\n",
    "            mult_channel_by = tensor(range(0, mask.size(0)))\n",
    "            mask = mask * mult_channel_by[:, None, None, None]\n",
    "            return torch.sum(mask, 0)  \n",
    "        else: return mask\n",
    "        \n",
    "    def match_size_mask_source(self):\n",
    "        \"rescales the segmentation mask to the original image resolution\"\n",
    "\n",
    "        if self.item_tfms == None: raise TypeError('No item_tfms specified.')\n",
    "        source = TensorDicom3D.create(self.item_path)\n",
    "        self.metadata = source.metadata\n",
    "        source = self.item_tfms(source) # will lose metadata in transforms\n",
    "        \n",
    "        x,y = source.size()[1:]\n",
    "        z = self.mask.size(0)\n",
    "        source = source.resize_3d(size = (z,x,y), mode = 'trilinear')\n",
    "        self.mask = self.mask.resize_3d(size = source.size(), mode = 'nearest')\n",
    "        self.mask = TensorMask3D(self.mask)\n",
    "        self.orig = TensorMask3D(source)\n",
    "        self.mask.metadata = self.metadata\n",
    "        self.orig.metadata = self.metadata\n",
    "\n",
    "    \n",
    "    def predict(self, item, rm_type_tfms=None):\n",
    "        self.item_path = item\n",
    "        _, self.mask, _ = self.model.predict(self.item_path, rm_type_tfms)\n",
    "        self.mask = self.mask.round()\n",
    "        self.mask = self.merge_mask(self.mask)\n",
    "        self.mask = TensorMask3D(self.mask)\n",
    "\n",
    "        self.match_size_mask_source()\n",
    "    \n",
    "    def show_pair(self, alpha = 0.25, **kwargs):\n",
    "        self.orig.show(**kwargs)\n",
    "        self.mask.show(add_to_existing = True, alpha = alpha, cmap = 'jet', **kwargs)\n",
    "        \n",
    "    def strip_pair(self, pad_z = 1, pad_xy = 5):\n",
    "        \"padds the idexes, so that a small margin of zeros remains\"\n",
    "\n",
    "        self.orig.strip_idx = self.mask.get_strip_idx(symmetric=True)\n",
    "        self.mask = self.mask.strip(pad_z = pad_z, pad_xy=pad_xy)\n",
    "        self.orig = self.orig.strip(pad_z = pad_z, pad_xy=pad_xy)\n",
    "        self.orig.metadata = self.metadata\n",
    "        self.mask.metadata = self.metadata\n",
    "        \n",
    "    def convert_and_export(self, source, orig_name, mask_name, pad_z, pad_xy, verbose):\n",
    "        \"\"\"\n",
    "        Reads a list of source images and uses a given learner to predict the mask. Crops the mask and source image an then exports the files. \n",
    "        \n",
    "        Args: \n",
    "            source: str or path. The path to the original image. Can be a DICOM direcetory or a single DICOM, NIfTI, NRRD, Analyze file (any type supported by SimpleITK) \n",
    "            orig_name: str or path. New filename for the cropped original image\n",
    "            mask_name: str or path. New filename for the predicted mask.\n",
    "            \n",
    "        returns: \n",
    "            Nothing. Writes files to disk.\n",
    "        \"\"\"\n",
    "        if self.model is None: raise NameError('No model for predictions assigned. Assing a model to the {} using {}.assign_model(model)'.format(self.__class__.__name__, self.__class__.__name__))\n",
    "        \n",
    "        self.predict(source)\n",
    "        self.strip_pair(pad_z = pad_z, pad_xy = pad_xy)\n",
    "        self.orig.save(orig_name)\n",
    "        self.mask.save(mask_name)\n",
    "        \n",
    "        if verbose: print('wrote image to {} and mask to {}'.format(str(orig_name), str(mask_name)))\n",
    "            \n",
    "    def convert_and_export_list(self, source, orig_name, mask_name, pad_z = 1, pad_xy = 5, verbose = True):\n",
    "        if not isinstance(source, list) or not isinstance(orig_name, list) or not isinstance(mask_name, list): raise TypeError('source, orig_name, mask_name need to be lists of equal size')\n",
    "        if len(source) != len(orig_name) or len(source) != len(mask_name): raise TypeError('source, orig_name, mask_name need to be lists of equal size')\n",
    "                     \n",
    "        for s, o, m, in zip(source, orig_name, mask_name): \n",
    "            self.convert_and_export(s, o, m, pad_z, pad_xy, verbose)            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A model needs to be assigned to the SubvolumeExporter, so predictions on new data can be made. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "SubvolumeExport = SubvolumeExporter() \n",
    "SubvolumeExport.assign_model(learn)\n",
    "SubvolumeExport.assign_tfms(RandomCrop3D(crop_by = (0.2, 0.15, 0.15), rand_crop_xyz = (0,0,0), perc_crop = True))\n",
    "SubvolumeExport.predict(d.t2_dcm_path[0]) # predict mask on one image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check the quality of the class and predictions, botch can be visualized.  "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "SubvolumeExport.show_pair(nrow = 16, figsize = (35, 25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using strip_paor reduces the image size, so that it narrowly fits the mask. \n",
    "Per default padding will be 0 so the largest mask diameter will decide on the size of the image. Using pad_z, pad_x, pad_y (or pad_xy instead of pad_x, pad_y) a small marign can be added in case the classes do not fit perfectly. Padding image size larger then original size is not possible and will not lead to black spaces.  "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "SubvolumeExport.strip_pair(pad_xy = 5, pad_z = 10)\n",
    "SubvolumeExport.show_pair(nrow = 16, figsize = (35, 25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce size of original image and mask\n",
    "\n",
    "The predicted mask and theroff derived images can be used for segementation pipelines, however the mask shows only as much imformation as the model did predict. To further train the images, it makes no sense exporting the predicted mask, but the original mask and images need to be cropped and exported. A subclass of `SubvolumeExporter` can convert the original files, but the export functions need to be adapted.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class CropOriginalToMask(SubvolumeExporter):\n",
    "    \n",
    "    def load_pair(self, image, mask):\n",
    "        self.orig = TensorMask3D.create(image)\n",
    "        self.mask = TensorMask3D.create(mask)\n",
    "        self.metadata = self.orig.metadata\n",
    "        \n",
    "        self.mask.metadata = self.metadata\n",
    "        \n",
    "    def export_pair(self, new_image, new_mask, pad_z, pad_xy):\n",
    "        self.strip_pair(pad_z = pad_z, pad_xy = pad_xy)\n",
    "        self.orig.save(new_image)\n",
    "        self.mask.save(new_mask)\n",
    "        \n",
    "    def convert_and_export(self, image, mask, new_image=None, new_mask=None, pad_z=1, pad_xy=10, verbose = True):\n",
    "        if isinstance(image, str): image = Path(image)\n",
    "        if isinstance(mask, str): mask = Path(mask)\n",
    "                \n",
    "        if new_image is None: \n",
    "            if image.is_dir(): \n",
    "                new_image = image/'cropped_volume.nii.gz'\n",
    "            else: \n",
    "                new_image = image.parent/'cropped_volume.nii.gz'\n",
    "                \n",
    "                \n",
    "        if new_mask is None: \n",
    "            if mask.is_dir(): \n",
    "                new_mask = mask/'cropped_mask.nii.gz'\n",
    "            else:     \n",
    "                new_mask = mask.parent/'cropped_mask.nii.gz'\n",
    "          \n",
    "        self.load_pair(image, mask)\n",
    "        self.export_pair(new_image, new_mask, pad_z, pad_xy)\n",
    "               \n",
    "        if verbose: print('wrote image to {} and mask to {}'.format(str(new_image), str(new_mask)))\n",
    "            \n",
    "    def convert_and_export_list(self, image, mask, new_image=None, new_mask=None,  pad_z = 1, pad_xy = 10, verbose = True):\n",
    "        if not isinstance(image, list) or not isinstance(mask, list): raise TypeError('source, orig_name, mask_name need to be lists of equal size')\n",
    "         \n",
    "        if len(image) != len(mask): raise TypeError('source, orig_name, mask_name need to be lists of equal size')\n",
    "        \n",
    "        if new_image is None: new_image = [None for i in image]\n",
    "        if new_mask is None: new_mask = [None for i in mask]\n",
    "            \n",
    "        \n",
    "        for i, m, ni, nm in zip(image, mask, new_image, new_mask): \n",
    "            self.convert_and_export(image=i, mask=m, new_image=ni, new_mask=nm, pad_z=pad_z, pad_xy=pad_xy, verbose=verbose)    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Crop = CropOriginalToMask()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Crop.convert_and_export(d.t2_dcm_path[0], d.t2_mask_base[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 01_basics.ipynb.\n",
      "Converted 02_preprocessing.ipynb.\n",
      "Converted 03_transforms.ipynb.\n",
      "Converted 04_dataloaders.ipynb.\n",
      "Converted 05_learner.ipynb.\n",
      "Converted 06a_models.alexnet.ipynb.\n",
      "Converted 06b_models.resnet.ipynb.\n",
      "Converted 06c_models.densenet.ipynb.\n",
      "Converted 06d_models.DynamicUnet.ipynb.\n",
      "Converted 06d_models.unet.ipynb.\n",
      "Converted 06e_models.deeplabv3.ipynb.\n",
      "Converted 06f_models.losses.ipynb.\n",
      "Converted 07_callback.ipynb.\n",
      "Converted 99_tools.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (fastai v2)",
   "language": "python",
   "name": "fastai-v2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
