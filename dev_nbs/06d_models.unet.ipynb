{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3D UNet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp models.unet\n",
    "# export\n",
    "from fastai.basics import *\n",
    "from faimed3d.basics import *\n",
    "from fastai.vision.all import create_body, hook_outputs\n",
    "from torchvision.models.video import r3d_18\n",
    "from fastai.vision.models.unet import DynamicUnet, _get_sz_change_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import faimed3d\n",
    "from faimed3d.layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "body_3d = create_body(r3d_18, pretrained = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Unet 3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fastais `DynamicUnet` allows construction of a UNet using any pretrained CNN as backbone/encoder. A key module is `nn.PixelShuffle` which allows subpixel convolutions for upscaling in the UNet Blocks. However, `nn.PixelShuffle` is only for 2D images, so in faimed3d `nn.ConvTranspose3d` is used instead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class ConvTranspose3D(nn.Sequential):\n",
    "    \"Upsample by 2` from `ni` filters to `nf` (default `ni`), using `nn.ConvTranspose3D`.\"\n",
    "    def __init__(self, ni, nf=None, scale=2, blur=False, act_cls=None, norm_type=None, **kwargs):\n",
    "        super().__init__()\n",
    "        nf = ifnone(nf, ni)\n",
    "        layers = [ConvLayer(ni, nf, ndim=3, act_cls=act_cls, norm_type=norm_type, transpose=True, **kwargs)]\n",
    "      #  layers[0].weight.data.copy_(icnr_init(layers[0].weight.data)) \n",
    "        if blur: layers += [nn.ReplicationPad3d((1,0,1,0,1,0)), nn.AvgPool3d(2, stride=1)]\n",
    "        super().__init__(*layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fastai's `PixelShuffle_ICNR` first performes a convolution to increase the layer size, then applies `PixelShuffle` to resize the image. A special initialization technique is applied to `PixelShuffle`, which can reduce checkerboard artifacts (see https://arxiv.org/pdf/1707.02937.pdf). It is probably not needed for `nn.ConvTranspose3d`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 5, 15, 15])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ConvTranspose3D(256, 128)(torch.randn((1, 256, 3, 13, 13))).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 5, 15, 15])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ConvTranspose3D(256, 128, blur = True)(torch.randn((1, 256, 3, 13, 13))).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To work with 3D data, the `UnetBlock` from fastai is adapted, replacing `PixelShuffle_ICNR` with the above created `ConvTranspose3D` and also adapting all conv-layers and norm-layers to the 3rd dimension. As small differences in size may appear, `forward`-func contains a interpolation step, which is also adapted to work with 5D input instead of 4D.  \n",
    "`UnetBlock3D` recieves the lower level features as hooks. In contrast to `fastai` here the hooks are a list of tensors with `len(hook) == n_inp`, where `n_inp` is the number of 3D sequences in a 4D dimensional input. **Important** 4D here does not refer to the dimensionality of the Tensor, where 4D would be B x C x H x W but to the dimensionality of the input **before** it was concatenated to a Tensor. 3D means we have one 3D volume with dimensionality (C) x D x H x W and 4D means we have multiple 3D volumes. In medical imaging this is not rare, as we often want to use information from multiple imaging sequences. \n",
    "The information of the different sequences can be assumed to be redundant to some kind, so in the `UnetBlock3D`, first the different feature maps from the hooks are concatenated and then pooled using a 1x1x1 convolutional layer. After this, the class is build similar to the fastai `UnetBlock`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class UnetBlock3D(Module):\n",
    "    \"A quasi-UNet block, using `ConvTranspose3d` for upsampling`.\"\n",
    "    @delegates(ConvLayer.__init__)\n",
    "    def __init__(self, up_in_c, x_in_c, hook, final_div=True, blur=False, act_cls=defaults.activation,\n",
    "                 self_attention=False, init=nn.init.kaiming_normal_, norm_type=None, **kwargs):\n",
    "        self.hook = hook\n",
    "        self.pool_fm = ConvLayer(x_in_c*len(hook.stored), x_in_c, ks = 1, ndim=3, act_cls=act_cls, norm_type=norm_type, **kwargs)\n",
    "        self.up = ConvTranspose3D(up_in_c, up_in_c//2, blur=blur, act_cls=act_cls, norm_type=norm_type, **kwargs)\n",
    "        self.bn = BatchNorm(x_in_c, ndim=3)\n",
    "        ni = up_in_c//2 + x_in_c\n",
    "        nf = ni if final_div else ni//2\n",
    "        self.conv1 = ConvLayer(ni, nf, ndim=3, act_cls=act_cls, norm_type=norm_type, **kwargs)\n",
    "        self.conv2 = ConvLayer(nf, nf, ndim=3, act_cls=act_cls, norm_type=norm_type,\n",
    "                               xtra=SelfAttention(nf) if self_attention else None, **kwargs)\n",
    "        self.relu = act_cls()\n",
    "        apply_init(nn.Sequential(self.conv1, self.conv2), init)\n",
    "\n",
    "    def forward(self, up_in):\n",
    "        \n",
    "        s = self.pool_fm(torch.cat(self.hook.stored, 1))\n",
    "        up_out = self.up(up_in)\n",
    "        ssh = s.shape[-3:]\n",
    "        if ssh != up_out.shape[-3:]:\n",
    "            up_out = F.interpolate(up_out, s.shape[-3:], mode='nearest')\n",
    "        cat_x = self.relu(torch.cat([up_out, self.bn(s)], dim=1))\n",
    "        return self.conv2(self.conv1(cat_x))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output size of the last Unet-Block can be slightly different than the original input size, so one of the lasts steps in `DynamicUnet` is `ResizeToOrig` which is also adapted to work with 5D instead of 4D input images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class ResizeToOrig(Module):\n",
    "    \"Merge a shortcut with the result of the module by adding them or concatenating them if `dense=True`.\"\n",
    "    def __init__(self, mode='nearest'): self.mode = mode\n",
    "    def forward(self, x):\n",
    "        if x.orig.shape[-3:] != x.shape[-3:]:\n",
    "            x = F.interpolate(x, x.orig.shape[-3:], mode=self.mode)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`SequentialEx` does not allow to pass more than one item to `forward`, so it is subclassed to allow tuples. Also the input needs to pass the first two blocks outside of the loop, as inputs are a tuple/list at this state. Block two concatenates the input to a single tensor, which can then be passed to the loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class SequentialEx4D(SequentialEx):\n",
    "    \"Like `SequentialEx`, but handels orig data differently and allows to pass a tuple/list as input\"\n",
    "    def forward(self, *inputs):\n",
    "        # can't assign attribute to tuple/list, so passing through encoder outside of loop\n",
    "        res = self.layers[0](tuple(inputs)) # encoder\n",
    "        res = self.layers[1](tuple(res)) # concat, after this res is not a list/tuple anymore\n",
    "        for l in self.layers[2:]:\n",
    "            res.orig = inputs[0] \n",
    "            nres = l(res)\n",
    "            # We have to remove res.orig to avoid hanging refs and therefore memory leaks\n",
    "            res.orig, nres[0].orig = None, None\n",
    "            res = nres\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `DynamicUnet3D` is very similar to fastai's `DynamicUnet`. Key differences are, that each `ConvLayer` or `BatchNorm` got an extra `ndim=3` argument and `UnetBlock` is replaced by `UnetBlock3D`.\n",
    "Furthermore, `n_dim` can be passed to the U-Net allowing the processing of four dimensional input. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`faimed3d` needs to support 4D data, that is multiple 3D inputs. The reason, a radiologist uses multiple sequences, is that some information is only present in a certain sequence. For example, to make the diagnosis of stroke, we need a strong hyperintense signal in the DWI and also a corresponding hypointense signal on the ADC map. It is (nearly) not possible to make a diagnosis only from one sequence. When we build a model, we also want it to have access to all relevant information.  \n",
    "The way fastai handels mulitple inputs, is to store them in a tuple. So if we have two 3D volumes as input and one mask as target, the batch will be as follows:  \n",
    "(TensorDicom3D of size B x 3 x D x H W, TensorDicom3D of size B x 3 x D x H x W, TensorMask3D of size B x 3 x D x H x W).\n",
    "`faimed3d` assumes, that all sequences are of roughly the same orientation (e.g. all axial) and also of the same region. So, information in the sequences can be assumed to be redundant to some extend and we likely do not need an encoder for each sequence and can re-use the weights of one encoder for all images. This approach saves us a lot of memory, however makes the training longe. However, it might be beneficial to still have some different weights for each sequence. For this reason, `faimed3d` splits a given encoder into it's stem and main body and duplicated the stem according to the number of inputs using the `MultiStem` class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class MultiStem(SequentialEx): \n",
    "    'applies one input of inputs to only one layer of layers'\n",
    "    def forward(self, inputs)->list:\n",
    "        out = []\n",
    "        for i, inp in enumerate(inputs):\n",
    "            out.append(self.layers[i](inp))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this, we want to send the feature maps from each stem through the same encoder. This is achieved with the `RepeatedSequential` class, which takes a list of tensors and applies it's modules to each element of the list. A list of Tensors is then returned again so that multiple instances of `RepeatedSequential` can be chained after each other. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class RepeatedSequential(SequentialEx):\n",
    "    'passes multiple inputs through the same neural network'\n",
    "    def forward(self, inputs) -> list:\n",
    "        return [module(inp) for module in self for inp in inputs]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main encoder for the UNet is build with `Arch4D`, which takes a encoder, splits the stem and body and convertes the stem to a `MultiStem` and the submodules of the body to `RepeatedSequential`. `Arch4D` can be indexed as the normal encoder and has the same number of subclasses. If outputs are hooked in `Arch4D` it will return a list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Arch4D(SequentialEx):\n",
    "    'repeatedly applies the same network to different inputs'\n",
    "    def __init__(self, arch, n_inp):\n",
    "        stems = MultiStem(*[arch[0] for _ in range(n_inp)]) # different stem for each input\n",
    "        body = [RepeatedSequential(l) for l in arch[1:]] # same body/shared weights for each input       \n",
    "        self.layers = nn.ModuleList([stems, *body])\n",
    "\n",
    "    def forward(self, inputs)->list:\n",
    "        for l in self.layers:\n",
    "            inputs = l(inputs)\n",
    "        return inputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Arch4D(body_3d, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, torch.Size([10, 512, 2, 4, 4]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = encoder((torch.randn(10, 3, 10, 50, 50), torch.randn(10, 3, 10, 50, 50)))\n",
    "len(out), out[0].size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`model_sizes` and `dummy_eval_4d` both need to be extendet to handle multiple inputs in form if lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def model_sizes(m, size=(8,64,64), n_inp=1):\n",
    "    \"Pass a dummy input through the model `m` to get the various sizes of activations. same as fastai func\"\n",
    "    with hook_outputs(m) as hooks:\n",
    "        _ = dummy_eval_4d(m, size=size, n_inp=n_inp)\n",
    "        return [o.stored[0].shape for o in hooks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def dummy_eval_4d(m, size=(8,64,64), n_inp=1):\n",
    "    \"Evaluate `m` on a dummy input of a certain `size`. Same as fastai func\"\n",
    "    ch_in = in_channels(m)\n",
    "    x = one_param(m).new(1, ch_in, *size).requires_grad_(False).uniform_(-1.,1.)\n",
    "    with torch.no_grad(): return m.eval()((x, )*n_inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([1, 64, 8, 32, 32]),\n",
       " torch.Size([1, 64, 8, 32, 32]),\n",
       " torch.Size([1, 128, 4, 16, 16]),\n",
       " torch.Size([1, 256, 2, 8, 8]),\n",
       " torch.Size([1, 512, 1, 4, 4])]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_sizes(encoder, n_inp = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Arch4D` returns a list of tensors, which needs to be concatenated for further processing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Concat(Module): \n",
    "    def __init__(self, ni, ndim, dim = 1):\n",
    "        store_attr()\n",
    "        self.bn = BatchNorm(ni, ndim)\n",
    "        self.act = nn.ReLU()\n",
    "        \n",
    "    def forward(self, inputs:(list, tuple))->Tensor: \n",
    "        inputs = torch.cat(inputs, self.dim)\n",
    "        return self.act(self.bn(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`DynamicUnet3D` is the main UNet class for `faimed3d` and is very similar to the `fastai` `DynamicUnet`. \n",
    "Key differences are the adaption to 3D and 4D inputs. In `fastai` `DynamicUnet` the feature maps are stored as hook, each time the size of the feature maps changes in the encoder. In `faimed3d` we can have multiple inputs but only one encoder and the hooks return a list of tensors. This is adressed by adapting `model_sizes`, `dummy_eval`, adding an extra concat-layer and adapting the `UNetBlock`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class DynamicUnet3D(SequentialEx4D):\n",
    "    \"Create a U-Net from a given architecture.\"\n",
    "    def __init__(self, encoder, n_out, img_size, n_inp=1, blur=False, blur_final=True, self_attention=False,\n",
    "                 y_range=None, last_cross=True, bottle=False, act_cls=defaults.activation,\n",
    "                 init=nn.init.kaiming_normal_, norm_type=None, **kwargs):\n",
    "        \n",
    "        encoder = Arch4D(encoder, n_inp)        \n",
    "        sizes = model_sizes(encoder, size=img_size, n_inp=n_inp) # return sizes * n_inp\n",
    "        sz_chg_idxs = list(reversed(_get_sz_change_idxs(sizes)))\n",
    "        \n",
    "        self.sfs = hook_outputs([encoder[i] for i in sz_chg_idxs], detach=False)\n",
    "        x = dummy_eval_4d(encoder, img_size, n_inp)\n",
    "        x = [x_.detach() for x_ in x]\n",
    "        ni = sizes[-1][1]\n",
    "\n",
    "        middle_conv = nn.Sequential(ConvLayer(ni*n_inp, ni*2, act_cls=act_cls, norm_type=norm_type, ndim = len(img_size), **kwargs),\n",
    "                                    ConvLayer(ni*2, ni, act_cls=act_cls, norm_type=norm_type, ndim = len(img_size), **kwargs)).eval()\n",
    "        \n",
    "        concat = Concat(ni*n_inp, ndim = len(img_size))\n",
    "        \n",
    "        x = middle_conv(concat(x))\n",
    "        \n",
    "        layers = [encoder, concat, middle_conv]\n",
    "        \n",
    "        \n",
    "        for i,idx in enumerate(sz_chg_idxs):\n",
    "            not_final = i!=len(sz_chg_idxs)-1\n",
    "            up_in_c, x_in_c = int(x.shape[1]), int(sizes[idx][1])\n",
    "            do_blur = blur and (not_final or blur_final)\n",
    "            sa = self_attention and (i==len(sz_chg_idxs)-3)\n",
    "            \n",
    "            unet_block = UnetBlock3D(up_in_c, x_in_c, self.sfs[i], final_div=not_final, blur=do_blur, self_attention=sa,\n",
    "                                     act_cls=act_cls, init=init, norm_type=norm_type, **kwargs).eval()\n",
    "            layers.append(unet_block)\n",
    "            x = unet_block(x)\n",
    "\n",
    "        ni = x.shape[1]\n",
    "        if img_size != sizes[0][-3:]: layers.append(ConvTranspose3D(ni))\n",
    "        layers.append(ResizeToOrig())\n",
    "        if last_cross:\n",
    "            layers.append(MergeLayer(dense=True))\n",
    "            ni += in_channels(encoder)\n",
    "            layers.append(ResBlock(1, ni, ni//2 if bottle else ni, act_cls=act_cls, norm_type=norm_type, ndim = 3, **kwargs))\n",
    "        layers += [ConvLayer(ni, n_out, ks=1, act_cls=None, norm_type=norm_type, ndim = 3, **kwargs)]\n",
    "        apply_init(nn.Sequential(layers[3], layers[-2]), init)\n",
    "        #apply_init(nn.Sequential(layers[2]), init)\n",
    "        if y_range is not None: layers.append(SigmoidRange(*y_range))\n",
    "        super().__init__(*layers)\n",
    "\n",
    "    def __del__(self):\n",
    "        if hasattr(self, \"sfs\"): self.sfs.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet3d = DynamicUnet3D(body_3d, 2, (10,50,50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 10, 50, 50])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unet3d(torch.rand(2, 3, 10, 50, 50)).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet4d = DynamicUnet3D(body_3d, 2, (10,50,50), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 10, 50, 50])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unet4d(torch.rand(2, 3, 10, 50, 50), torch.rand(2, 3, 10, 50, 50), torch.rand(2, 3, 10, 50, 50), torch.rand(2, 3, 10, 50, 50)).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 01_basics.ipynb.\n",
      "Converted 02_preprocessing.ipynb.\n",
      "Converted 03_transforms.ipynb.\n",
      "Converted 04_dataloaders.ipynb.\n",
      "Converted 05_layers.ipynb.\n",
      "Converted 06_learner.ipynb.\n",
      "Converted 06a_models.alexnet.ipynb.\n",
      "Converted 06b_models.resnet.ipynb.\n",
      "Converted 06d_models.unet.ipynb.\n",
      "Converted 06f_models.losses.ipynb.\n",
      "Converted 07_callback.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (fastai v2)",
   "language": "python",
   "name": "fastai-v2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
