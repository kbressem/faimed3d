# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/06e_models.deeplabv3.ipynb (unless otherwise specified).

__all__ = ['ASPPPooling', 'ASPP', 'DeepLabDecoder', 'DynamicDeepLab']

# Cell
# export
from fastai.basics import *
from fastai.vision.all import create_body, hook_outputs
from fastai.vision.models.unet import _get_sz_change_idxs
from ..basics import *
from ..layers import *
from .unet import ResizeToOrig

# Cell
class ASPPPooling(nn.Sequential):
    "Pooling Layer for ASPP"
    def __init__(self, ni, nf, norm_type=None, act_cls=defaults.activation):
        super(ASPPPooling, self).__init__(
            nn.AdaptiveAvgPool3d(1),
            ConvLayer(ni=ni, nf=nf, ks=1, ndim=3, bias=False, norm_type=None, act_cls=act_cls)
        )

    def forward(self, x):
        size = x.shape[-3:]
        for module in self:
            x = module(x)
        return F.interpolate(x, size=size, mode='trilinear', align_corners=False)


class ASPP(SequentialEx):
    "3D Atrous Spatial Pyramid Pooling"
    def __init__(self, ni, dilations, nf, norm_type=None, act_cls=defaults.activation, ps=0.5):
        conv_layers = [ConvLayer(ni=ni, nf=nf, ks=1, bias=False, ndim=3, norm_type=norm_type, act_cls=act_cls)]
        dilations = tuple(dilations)
        for dilation in dilations:
            conv_layers.append(ConvLayer(ni=ni, nf=nf, ndim=3, ks=3, dilation=dilation, padding=dilation,
                                         norm_type=norm_type, act_cls=act_cls))
        pooling = ASPPPooling(ni=ni, nf=nf, norm_type=norm_type, act_cls=act_cls)
        self.layers = nn.ModuleList([*conv_layers, pooling])
        self.project = nn.Sequential(
            ConvLayer(ni=len(self.layers)*nf, nf=nf, ks=1, bias=False, ndim=3,
            norm_type=norm_type, act_cls=act_cls),
            nn.Dropout(ps))

    def forward(self, x):
        res = [module(x) for module in self.layers]
        return self.project(torch.cat(res, dim=1))

# Cell
class DeepLabDecoder(Module):
    "Decoder Block for DynamicDeeplab"
    def __init__(self, ni, low_lvl_ni, hook, n_out, norm_type=None,
                 act_cls=defaults.activation, ps=0.5):
        self.hook = hook

        self.low_lvl_conv = ConvLayer(low_lvl_ni, low_lvl_ni//2, ks=1, ndim=3, bias=False,
                                      norm_type=norm_type, act_cls=act_cls)

        self.last_conv = nn.Sequential(
                ConvLayer(ni+low_lvl_ni//2, ni, ks=3, ndim=3, stride=1, padding=1, bias=False,
                          norm_type=norm_type, act_cls=act_cls),
                nn.Dropout(ps),
                ConvLayer(ni, ni, ks=3, ndim=3, stride=1, padding=1, bias=False,
                          norm_type=norm_type, act_cls=act_cls),
                nn.Dropout(ps/5),
                nn.Conv3d(ni, n_out, kernel_size=1, stride=1))

    def forward(self, x):
        s = self.low_lvl_conv(sum(self.hook.stored))
        ssh = s.shape[-3:]
        if ssh != x.shape[-3:]:
            x = F.interpolate(x, size=ssh, mode='nearest')
        x = torch.cat((x, s), dim=1)
        return self.last_conv(x)


# Cell
class DynamicDeepLab(SequentialEx):
    "Build DeepLab with different encoders"
    def __init__(self, encoder, n_out, img_size, y_range=None,
                       act_cls=defaults.activation, norm_type=NormType.Batch, **kwargs):

        sizes = model_sizes(encoder, size=img_size)
        sz_chg_idxs = list(_get_sz_change_idxs(sizes))
        self.sfs = hook_outputs(encoder[sz_chg_idxs[1]], detach=False)
        x = dummy_eval(encoder, img_size).detach()
        ni = sizes[-1][1]
        nf = ni//4
        dilations=[1, 12, 24, 36] if ni > 512 else [1, 6, 12, 18]
        aspp = ASPP(ni=ni, nf=nf, dilations=dilations, norm_type=norm_type, act_cls=act_cls).eval()
        x = aspp(x)
        decoder = DeepLabDecoder(ni=nf, low_lvl_ni=sizes[sz_chg_idxs[1]][1], hook=self.sfs, n_out=n_out,
                                 norm_type=norm_type, act_cls=act_cls).eval()
        x = decoder(x)
        self.layers = nn.ModuleList([encoder, aspp, decoder, ResizeToOrig()])

    def __del__(self):
        if hasattr(self, "sfs"): self.sfs.remove()