{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Callbacks\n",
    "\n",
    "Special callbacks for 3D data or training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# default_exp callback\n",
    "\n",
    "from fastai.basics import *\n",
    "from fastai.vision.all import *\n",
    "from fastai.callback.all import *\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from faimed3d.basics import *\n",
    "from faimed3d.preprocess import *\n",
    "from faimed3d.augment import *\n",
    "from faimed3d.data import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Channel manipulation\n",
    "\n",
    "For processing multiple 3D volumes, the volumes can be stacked to the color dimension. This need to be implemented as callback before the batch is presented to the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class StackVolumes(Callback):\n",
    "    \"\"\"\n",
    "    Takes multiple 3D volumes and stacks them in the channel dim.\n",
    "    This is useful when using multi-sequence medical data.\n",
    "\n",
    "    Example:\n",
    "        Having the Tensors of size (10, 1, 5, 25, 25) would lead to a single Tensor of\n",
    "        size (10, 3, 5, 25, 25).\n",
    "    \"\"\"\n",
    "\n",
    "    def before_batch(self):\n",
    "        self.learn.xb = (torch.cat(self.learn.xb, dim=1), )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks for volume manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In medical imaging, small regions in the image are often decisive for the diagnosis. This means, given a smaller subregion of the image, the model could still correctly detect the pathology. Through splitting the volumes the data might thus be augmented. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class SplitVolumes(Callback):\n",
    "    \"\"\"\n",
    "        Separates a 3D tensor into smaller equal-sized sub-volumes.\n",
    "\n",
    "         o---o---o       o---o---o\n",
    "         | A | A |       | B | B |        o---o  o---o  o---o  o---o  o---o  o---o  o---o  o---o\n",
    "         o---o---o   +   o---o---o  ==>   | A | +| A | +| B | +| B | +| A | +| A | +| B | +| B |\n",
    "         | A | A |       | B | B |        o---o  o---o  o---o  o---o  o---o  o---o  o---o  o---o\n",
    "         o---o---o       o---o---o\n",
    "\n",
    "\n",
    "        Args:\n",
    "            n_subvol = number of subvolumes\n",
    "            split_along_depth = whether volumes should also be split along the D dimension fpr a [B, C, D, H, W] tensor\n",
    "    \"\"\"\n",
    "    run_after = StackVolumes\n",
    "    def __init__(self, n_subvol = 2**3, split_along_depth = True):\n",
    "        store_attr()\n",
    "\n",
    "    def before_batch(self):\n",
    "        xb = self.learn.xb\n",
    "        if len(xb) > 1: raise ValueError('Got multiple items in x batch. You need to concatenate the batch first.')\n",
    "        self.learn.xb = self.split_volume(xb)\n",
    "        self.learn.yb = self.split_volume(self.learn.yb)\n",
    "\n",
    "    def after_pred(self):\n",
    "        self.learn.xb = self.patch_volume(self.learn.xb)\n",
    "        self.learn.pred = detuplify(self.patch_volume(self.learn.pred))\n",
    "        self.learn.yb = self.patch_volume(self.learn.yb)\n",
    "\n",
    "    def split_volume(self, xb:(Tensor, TensorDicom3D, TensorMask3D)):\n",
    "        \"splits a large tensor into multiple smaller tensors\"\n",
    "\n",
    "        xb = detuplify(xb) # xb is always a tuple\n",
    "        # calculate number of splits per dimension\n",
    "        self.n = self.n_subvol**(1/3) if self.split_along_depth else self.n_subvol**0.5\n",
    "        self.n = int(self.n)\n",
    "\n",
    "        # check if shape of dims is divisible by n, if not resize the Tensor acordingly\n",
    "        shape = [s if s % self.n == 0 else s - s % self.n for s in xb.shape[-3:]]\n",
    "        if not self.split_along_depth: shape[0]=xb.shape[0]\n",
    "        xb = F.interpolate(xb, size = shape, mode = \"trilinear\", align_corners=True)\n",
    "\n",
    "        # split each dim into smaller dimensions\n",
    "        d, h, w = shape\n",
    "        if self.split_along_depth: xb = xb.reshape(xb.size(0), xb.size(1), self.n, int(d/self.n), self.n, int(h/self.n), self.n, int(w/self.n))\n",
    "        else: xb = xb.reshape(xb.size(0), xb.size(1),1, d, self.n, int(h/self.n), self.n, int(w/self.n))\n",
    "\n",
    "        # swap the dimensions an flatten Batchdim and the newly created dims\n",
    "        # return a tuple as xb is always a tuple\n",
    "        return (xb.permute(1, 3, 5, 7, 0, 2, 4, 6).flatten(-4).permute(4, 0, 1, 2, 3), )\n",
    "\n",
    "    def patch_volume(self, p:(Tensor, TensorDicom3D, TensorMask3D)):\n",
    "        \"patches a prior split volume back together\"\n",
    "        p = detuplify(p)\n",
    "\n",
    "        old_shape = p.shape[0]//self.n_subvol, p.shape[1], *[s * self.n for s in p.shape[2:]]\n",
    "        if not self.split_along_depth: old_shape[2]=p.shape[2]\n",
    "        p = p.reshape(old_shape[0], self.n, self.n, self.n, *p.shape[1:])\n",
    "        return (p.permute(0, 4, 1, 5, 2, 6, 3, 7).reshape(old_shape), )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subsampling the subvolumes allows for more variability in the image and also training with a batch size < 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class SubsampleShuffle(SplitVolumes):\n",
    "    \"\"\"\n",
    "        After splitting the volume into multiple subvolumes, draws a randon amount of subvolumes for training.\n",
    "        Would allow to train on an effective batch size < 1.\n",
    "\n",
    "        o---o---o        o---o---o\n",
    "        | A | A |        | B | B |        o---o  o---o  o---o  o---o  o---o  o---o\n",
    "        o---o---o    +   o---o---o  ==>   | B | +| A | +| A | +| A | +| B | +| A |\n",
    "        | A | A |        | B | B |        o---o  o---o  o---o  o---o  o---o  o---o\n",
    "        o---o---o        o---o---o\n",
    "\n",
    "        Args:\n",
    "            p: percentage of subvolumes to train on\n",
    "    \"\"\"\n",
    "    run_after = [StackVolumes]\n",
    "\n",
    "    def __init__(self, p = 0.5, n_subvol=2**3, split_along_depth = True):\n",
    "        store_attr()\n",
    "\n",
    "    def before_batch(self):\n",
    "\n",
    "        xb = self.learn.xb\n",
    "        if len(xb) > 1: raise ValueError('Got multiple items in x batch. You need to concatenate the batch first.')\n",
    "        self.learn.xb = self.split_volume(xb)\n",
    "        self.learn.yb = self.split_volume(self.learn.yb)\n",
    "\n",
    "        if self.training:\n",
    "            xb = detuplify(self.learn.xb)\n",
    "            yb = detuplify(self.learn.yb)\n",
    "            draw = tuple(random.sample(range(0, xb.size(0)), int(xb.size(0)*self.p)))\n",
    "            self.learn.xb = (xb[draw, :], )\n",
    "            self.learn.yb = (yb[draw, :], )\n",
    "\n",
    "    def after_pred(self):\n",
    "        if not self.training:\n",
    "            self.learn.xb = self.patch_volume(self.learn.xb)\n",
    "            self.learn.pred = detuplify(self.patch_volume(self.learn.pred))\n",
    "            self.learn.yb = self.patch_volume(self.learn.yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming, a small finding is predominantly located in the, e.g. upper left image region, the model might wrongly learn the location as an important factor for the finding. Mixing subvolumes might help. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class MixSubvol(SplitVolumes):\n",
    "    \"\"\"\n",
    "        After splitting the volume into multiple subvolumes, shuffels the subvolumes and sticks the images back together.\n",
    "\n",
    "        o---o---o        o---o---o        o---o---o        o---o---o\n",
    "        | A | A |        | B | B |        | B | B |        | A | B |\n",
    "        o---o---o    +   o---o---o  ==>   o---o---o    +   o---o---o\n",
    "        | A | A |        | B | B |        | A | A |        | B | A |\n",
    "        o---o---o        o---o---o        o---o---o        o---o---o\n",
    "\n",
    "\n",
    "        Args:\n",
    "            p: probability that the callback will be applied\n",
    "            n_subvol: number of subvolumina to create\n",
    "            split_along_depth: whether the depth dimension should be included\n",
    "\n",
    "    \"\"\"\n",
    "    run_after = [StackVolumes]\n",
    "\n",
    "    def __init__(self, p = 0.25, n_subvol=2**3, split_along_depth = True):\n",
    "        store_attr()\n",
    "\n",
    "    def before_batch(self):\n",
    "        if self.training and random.random() < self.p:\n",
    "            xb = self.learn.xb\n",
    "            if len(xb) > 1: raise ValueError('Got multiple items in x batch. You need to concatenate the batch first.')\n",
    "            xb = detuplify(self.split_volume(xb))\n",
    "            yb = detuplify(self.split_volume(self.learn.yb))\n",
    "            shuffle = tuple(random.sample(range(0, xb.size(0)), xb.size(0)))\n",
    "            self.learn.xb = self.patch_volume((xb[shuffle, :], ))\n",
    "            self.learn.yb = self.patch_volume((yb[shuffle, :], ))\n",
    "\n",
    "    def after_pred(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation for MixUp on 3D data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracker callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class ReloadBestFit(TrackerCallback):\n",
    "    \"A `TrackerCallback` that reloads the previous best model if not improvement happend for n epochs\"\n",
    "    def __init__(self, fname,  monitor='valid_loss', comp=None, min_delta=0., patience=1):\n",
    "        super().__init__(monitor=monitor, comp=comp, min_delta=min_delta)\n",
    "        self.patience = patience\n",
    "        self.fname = fname\n",
    "\n",
    "    def before_fit(self): self.wait = 0; super().before_fit()\n",
    "    def after_epoch(self):\n",
    "        \"Compare the value monitored to its best score and maybe stop training.\"\n",
    "        super().after_epoch()\n",
    "        if self.new_best: self.wait = 0\n",
    "        else:\n",
    "            self.wait += 1\n",
    "            if self.wait >= self.patience:\n",
    "                print(f'No improvement since epoch {self.epoch-self.wait}: reloading previous best model.')\n",
    "                self.learn = self.learn.load(self.fname)\n",
    "                self.wait=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 01_basics.ipynb.\n",
      "Converted 02_preprocessing.ipynb.\n",
      "Converted 03_transforms.ipynb.\n",
      "Converted 04_dataloaders.ipynb.\n",
      "Converted 05_layers.ipynb.\n",
      "Converted 06_learner.ipynb.\n",
      "Converted 06a_models.alexnet.ipynb.\n",
      "Converted 06b_models.resnet.ipynb.\n",
      "Converted 06d_models.unet.ipynb.\n",
      "Converted 06f_models.losses.ipynb.\n",
      "Converted 07_callback.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai",
   "language": "python",
   "name": "fastai"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
