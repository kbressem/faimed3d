{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Losses and metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp models.losses\n",
    "# export \n",
    "from fastai.basics import *\n",
    "import torchvision, torch\n",
    "from warnings import warn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss functions\n",
    "\n",
    "### DICE Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class DiceLossBinary():\n",
    "    \n",
    "    \"\"\"\n",
    "    Simple DICE loss as described in: \n",
    "        https://arxiv.org/pdf/1911.02855.pdf    \n",
    "    \n",
    "    Computes the Sørensen–Dice loss. Larger is better. \n",
    "    Note that PyTorch optimizers minimize a loss. So the loss is subtracted from 1. \n",
    "    \n",
    "    Args:\n",
    "        targ:    A tensor of shape [B, 1, D, H, W].\n",
    "        pred:    A tensor of shape [B, 1, D, H, W]. Corresponds to\n",
    "                 the raw output or logits of the model.\n",
    "        method:  The method, how the DICE score should be calcualted. \n",
    "                    \"simple\"   = standard DICE loss\n",
    "                    \"miletari\" = squared denominator for faster convergence\n",
    "                    \"tversky\"  = variant of the DICE loss which allows to weight FP vs FN. \n",
    "        alpha, beta: weights for FP and FN for \"tversky\" loss, if both values are 0.5 the \n",
    "                 \"tversky\" loss corresponds to the \"simple\" DICE loss\n",
    "        smooth:  Added smoothing factor. \n",
    "        eps: added to the denominator for numerical stability (acoid division by 0).\n",
    "    Returns:\n",
    "        dice_loss: the Sørensen–Dice loss.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, method = 'miletari', alpha = 0.5, beta = 0.5, eps = 1e-7, smooth = 1.) -> None:\n",
    "        store_attr()\n",
    "    \n",
    "    def __call__(self, input: Tensor, target: Tensor) -> Tensor:\n",
    "        if input.min() < 0 or input.max() > 1: \n",
    "            warn(\"Input is not in range between 0 and 1 but the loss will work better with input in that range. Consider rescaling your input. \")\n",
    "           \n",
    "        dims = (0,) + tuple(range(2, target.ndim))\n",
    "\n",
    "        if self.method == 'simple':\n",
    "            numerator  = torch.sum(input * target, dims) + self.smooth\n",
    "            denominator  = torch.sum(input + target, dims) + self.smooth\n",
    "            dice_loss = (2. * numerator / (denominator + self.eps))\n",
    "\n",
    "        elif self.method == 'miletari':  \n",
    "            numerator  = torch.sum(input * target, dims) + self.smooth\n",
    "            denominator  = torch.sum(input**2 + target**2, dims) + self.smooth\n",
    "            dice_loss = (2. * numerator / (denominator + self.eps))\n",
    "\n",
    "        elif self.method == 'tversky':\n",
    "            numerator  = torch.sum(input * target, dims) + self.smooth\n",
    "            fps = torch.sum(input * (1 - target), dims)\n",
    "            fns = torch.sum((1 - input) * target, dims)\n",
    "\n",
    "            denominator  = numerator + self.alpha*fps + self.beta*fns + self.smooth\n",
    "            dice_loss = (2. * numerator / (denominator + self.eps))\n",
    "            \n",
    "        else: \n",
    "            raise NotImplementedError('The specified type of DICE loss is not implemented')\n",
    "\n",
    "        return 1-dice_loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export \n",
    "class DiceLossMulti(DiceLossBinary):\n",
    "    def __init__(self, n_classes, weights=None, **kwargs):\n",
    "        store_attr()\n",
    "        super().__init__(**kwargs)\n",
    "    \n",
    "    def __call__(self, input:Tensor, target:Tensor) -> Tensor:\n",
    "        self.get_weights(input)\n",
    "\n",
    "        if target.size(1) == 1:\n",
    "            target = self.to_one_hot(target)\n",
    "        elif target.size(1) != input.size(1):\n",
    "            raise ValueError(\"Number of Channels between input and target do not match.\"\n",
    "                             \"Expected target to have 1 or {} channels but got {}\".format(input.size(1), target.size(1)))\n",
    "        input = self.activation(input)\n",
    "        return torch.mean(super().__call__(input, target)*self.weights)\n",
    "    \n",
    "    def get_weights(self, target):\n",
    "        if self.weights == 'auto':\n",
    "            \"estimates weights from the percentage distribution of a finding.\"\n",
    "            dims = (0,) + tuple(range(2, target.ndim))\n",
    "            self.weights = 1/torch.mean(target, dims)\n",
    "        elif self.weights == None:\n",
    "            self.weights = 1.\n",
    "        elif isinstance(self.weights, (tuple, list)):\n",
    "            self.weights = tensor(self.weights)\n",
    "\n",
    "    def make_binary(self, t, set_to_one):\n",
    "        return (t == set_to_one).float()\n",
    "\n",
    "    def to_one_hot(self, target:Tensor):\n",
    "        target = target.squeeze(1).long() # remove the solitary color channel (if there is one) and set type to int64\n",
    "        one_hot = [self.make_binary(target, set_to_one=i) for i in range(0, self.n_classes)]\n",
    "\n",
    "        return torch.stack(one_hot, 1)\n",
    "\n",
    "    def activation(self, input):\n",
    "        return F.softmax(input, 1)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MCC Loss\n",
    "\n",
    "Implementing the MCC score as loss function:  \n",
    "\n",
    "\n",
    "$$\\frac{ \n",
    "        \\sum_{i}^{n} p_{ i }g_{ i } * \\sum_{i}^{n}  1-p_{ i } 1-g_{ i } +\n",
    "        \\sum_{i}^{n}  1-p_{ i } g_{ i } * \\sum_{i}^{n}  p_{ i } 1-g_{ i }}{ \\sqrt{ \n",
    "        (\\sum_{i}^{n}  p_{ i } g_{ i } + \\sum_{i}^{n}  1-p_{ i } g_{ i }) * \n",
    "        (\\sum_{i}^{n}  p_{ i } g_{ i } + \\sum_{i}^{n} p_{ i } 1-g_{ i }) *  \n",
    "        (\\sum_{i}^{n}  1-p_{ i } g_{ i } + \\sum_{i}^{n} 1-p_{ i } 1-g_{ i }) * \n",
    "        (\\sum_{i}^{n}  p_{ i } 1-g_{ i } + \\sum_{i}^{n} 1-p_{ i } 1-g_{ i }) \n",
    "     } }$$\n",
    "\n",
    "where p_i is the prediction for pixel i and g_i the corresponding ground truth pixel and gamma is the smoothing factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class MCCLossBinary(DiceLossBinary):\n",
    "    \n",
    "    \"\"\"\n",
    "    Computes the MCC loss. \n",
    "    \n",
    "    From Wikipedia (https://en.wikipedia.org/wiki/Matthews_correlation_coefficient):\n",
    "        > The coefficient takes into account true and false positives and negatives and is generally \n",
    "        > regarded as a balanced measure which can be used even if the classes are of very different sizes\n",
    "        > The MCC is in essence a correlation coefficient between the observed \n",
    "        > and predicted binary classifications; it returns a value between −1 and +1. \n",
    "        > A coefficient of +1 represents a perfect prediction, 0 no better than random prediction\n",
    "        > and −1 indicates total disagreement between prediction and observation    \n",
    "    \n",
    "    For this loss to work best, the input should be in range 0-1, e.g. enforced through a sigmoid or softmax. \n",
    "    Note that PyTorch optimizers minimize a loss. So the loss is subtracted from 1. \n",
    "\n",
    "    Math: \n",
    "        \\frac{ \n",
    "            \\sum_{i}^{n} p_{ i }g_{ i } * \\sum_{i}^{n}  1-p_{ i } 1-g_{ i } +\n",
    "            \\sum_{i}^{n}  1-p_{ i } g_{ i } * \\sum_{i}^{n}  p_{ i } 1-g_{ i }}{ \\sqrt{ \n",
    "            (\\sum_{i}^{n}  p_{ i } g_{ i } + \\sum_{i}^{n}  1-p_{ i } g_{ i }) * \n",
    "            (\\sum_{i}^{n}  p_{ i } g_{ i } + \\sum_{i}^{n} p_{ i } 1-g_{ i }) *  \n",
    "            (\\sum_{i}^{n}  1-p_{ i } g_{ i } + \\sum_{i}^{n} 1-p_{ i } 1-g_{ i }) * \n",
    "            (\\sum_{i}^{n}  p_{ i } 1-g_{ i } + \\sum_{i}^{n} 1-p_{ i } 1-g_{ i }) \n",
    "         } }\n",
    "\n",
    "    Args:\n",
    "        input:   A tensor of shape [B, 1, D, H, W]. Predictions. \n",
    "        target:  A tensor of shape [B, 1, D, H, W]. Ground truth. \n",
    "        smooth:  Smoothing factor, default is 1. Inherited from DiceLossBinary base class \n",
    "        eps:     Added for numerical stability.\n",
    "    Returns:\n",
    "        mmc_loss: loss based on Matthews correlation coefficient\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "    \n",
    "    def __call__(self, input: Tensor, target: Tensor) -> Tensor:         \n",
    "        return self.compute_loss(input, target)\n",
    "    \n",
    "    def activation(self, input):\n",
    "        return torch.sigmoid(input)\n",
    "       \n",
    "    def compute_loss(self, input: Tensor, target: Tensor):\n",
    "        \n",
    "        dims = (0,) + tuple(range(2, target.ndim))\n",
    "        \n",
    "        tps = torch.sum(self.activation(input) * target, dims) \n",
    "        fps = torch.sum(self.activation(input) * (1 - target), dims)\n",
    "        fns = torch.sum((1 - self.activation(input)) * target, dims)\n",
    "        tns = torch.sum((1 - self.activation(input)) * (1-target), dims)\n",
    "            \n",
    "        numerator = (tps * tns - fps * fns) + self.smooth \n",
    "        denominator =  ((tps + fps) * (tps + fns) * (fps + tns) * (tns + fns) + self.eps)**0.5 + self.smooth\n",
    "        \n",
    "        mcc_loss = numerator / (denominator)\n",
    "        \n",
    "        return 1-mcc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0011])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = torch.randn(5,1,2,25,25).sigmoid()\n",
    "t = torch.randn(5,1,2,25,25).sigmoid().round()\n",
    "\n",
    "MCCLossBinary()(i, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class MCCLossMulti(MCCLossBinary):\n",
    "    \n",
    "    \"\"\"\n",
    "    Computes the MCC loss for a multilabel target. Basically the same as `MCCLossBinary` \n",
    "    but one hot encodes the target before computation. \n",
    "    \n",
    "    Args:\n",
    "        num_features: Number of different features in y. \n",
    "                 Must correspond to the maximum number of overall features in the whole dataset.\n",
    "        input:   A tensor of shape [B, C, D, H, W], where the `n_classes` should correspond to C.\n",
    "        target:  A tensor of shape [B, 1, D, H, W] or [B, C, D, H, W] where C is the same size as in the input.  \n",
    "        weights: Either a str: 'auto' for autocalculation, None or a list/tuple of soecified weights\n",
    "        smooth:  Smoothing factor, default is 1. Inherited from DiceLossBinary base class \n",
    "        eps:     Added for numerical stability.\n",
    "        n_classes: number of classes to predict\n",
    "    Returns:\n",
    "        mcc_loss: loss based on Matthews correlation coefficient\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, n_classes, weights=None, **kwargs):\n",
    "        store_attr()\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "    def __call__(self, input: Tensor, target: Tensor) -> Tensor:\n",
    "        \n",
    "        self.get_weights(input)\n",
    "        \n",
    "        if target.size(1) == 1:\n",
    "            target = self.to_one_hot(target)\n",
    "        elif target.size(1) != input.size(1):\n",
    "            raise ValueError(\"Number of Channels between input and target do not match.\"\n",
    "                             \"Expected target to have 1 or {} channels but got {}\".format(input.size(1), target.size(1)))\n",
    "            \n",
    "        return torch.mean(super().__call__(input, target)*self.weights)\n",
    "    \n",
    "    def get_weights(self, target):\n",
    "        if self.weights == 'auto': \n",
    "            \"estimates weights from the percentage distribution of a finding.\"\n",
    "            dims = (0,) + tuple(range(2, target.ndim))\n",
    "            self.weights = 1/torch.mean(target, dims)\n",
    "        elif self.weights == None: \n",
    "            self.weights = 1.\n",
    "        elif isinstance(self.weights, (tuple, list)):\n",
    "            self.weights = tensor(self.weights)\n",
    "    \n",
    "    def make_binary(self, t, set_to_one):\n",
    "        return (t == set_to_one).float()\n",
    "  \n",
    "    def to_one_hot(self, target:Tensor):\n",
    "        target = target.squeeze(1).long() # remove the solitary color channel (if there is one) and set type to int64\n",
    "        one_hot = [self.make_binary(target, set_to_one=i) for i in range(0, self.n_classes)]\n",
    "        return torch.stack(one_hot, 1)\n",
    "    \n",
    "    def activation(self, input): \n",
    "        return F.softmax(input, 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0020)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = torch.randn(5,5,2,25,25)\n",
    "t = torch.randint(0, 5, (5,1,2,25,25))\n",
    "\n",
    "MCCLossMulti(5)(i, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class SoftMCCLossMulti(MCCLossMulti):\n",
    "    \"\"\"\n",
    "    Same as MCCLossMulti but can handle float values. \n",
    "    Example: \n",
    "        t = torch.randn(2,5); t\n",
    "        >>> tensor([[ 0.9113, -0.7525, -2.1771, -0.2420, -0.2245],\n",
    "                    [ 1.9503, -1.2903,  0.1201,  0.2830,  0.0473]])\n",
    "                   \n",
    "        MCCLossMulti(2).make_binary(t, 1)\n",
    "        >>> tensor([[0., 0., 0., 0., 0.],\n",
    "                    [0., 0., 0., 0., 0.]])\n",
    "        \n",
    "        SoftMCCLossMulti(2).soft_binary(t, 0)\n",
    "        >>> tensor([[0.9113, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "                    [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]])\n",
    "    \"\"\"\n",
    "    \n",
    "    def soft_binary(self, t, set_to_one):\n",
    "        return torch.where(t.gt(set_to_one - 0.49) != t.gt(set_to_one + 0.49), \n",
    "                           t.float(), \n",
    "                           tensor(0.).to(t.device) if set_to_one > 0 else tensor(1.).to(t.device))\n",
    "    \n",
    "    def to_one_hot(self, target:Tensor):\n",
    "        target = target.squeeze(1) # remove the solitary color channel (if there is one) and set type to int64\n",
    "        one_hot = [self.soft_binary(target, set_to_one=i) for i in range(0, self.n_classes)]\n",
    "        return torch.stack(one_hot, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0059)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SoftMCCLossMulti(5)(i, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class WeightedMCCLossMulti(MCCLossMulti):\n",
    "    \"\"\"\n",
    "    Weighted version of `MCCLossMulti`. \n",
    "    Note that class specific weight can still be added through `weights` during initialization. \n",
    "    \n",
    "    Args: \n",
    "        alpha: weight for true positives\n",
    "        beta: weight for false positives\n",
    "        gamma: weight for false negatives\n",
    "        delta: weight for true negatives\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, gamma=0.5, delta=0.5,*args, **kwargs):\n",
    "        \"alpha and beta are already inherited from `DiceLossBinary`\"\n",
    "        store_attr()\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def compute_loss(self, input: Tensor, target: Tensor):\n",
    "\n",
    "        dims = (0,) + tuple(range(2, target.ndim))\n",
    "\n",
    "        tps = torch.sum(self.activation(input) * target, dims) \n",
    "        fps = torch.sum(self.activation(input) * (1 - target), dims) \n",
    "        fns = torch.sum((1 - self.activation(input)) * target, dims)\n",
    "        tns = torch.sum((1 - self.activation(input)) * (1-target), dims)\n",
    "\n",
    "        numerator = (tps * tns - fps * fns) + self.smooth\n",
    "        denominator =  ((tps * self.alpha + fps * self.beta) * (tps * self.alpha + fns * self.gamma) * (fps * self.beta + tns * self.delta) * (tns * self.delta + fns * self.gamma) + self.eps)**0.5 + self.smooth\n",
    "\n",
    "        mcc_loss = numerator / (denominator)\n",
    "\n",
    "        return 1-mcc_loss\n",
    "    \n",
    "    def activation(self, x): \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class MCCScore(MCCLossMulti):\n",
    "    def __init__(self, n_classes = None, thres = 0.5, **kwargs):\n",
    "        super().__init__(n_classes, **kwargs)\n",
    "        \n",
    "        self.n_classes = 1 if n_classes is None else n_classes\n",
    "        self.thres = thres\n",
    "    \n",
    "    def __call__(self, input:Tensor , target: Tensor):\n",
    "        if self.n_classes is not None: \n",
    "            target = self.to_one_hot(target)\n",
    "            \n",
    "            return 1-torch.mean(super().__call__(input, target))\n",
    "        \n",
    "    def activation(self, input): \n",
    "        return (input > self.thres).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0062)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MCCScore()(i, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 01-basics.ipynb.\n",
      "Converted 02-transforms.ipynb.\n",
      "Converted 03-datablock.ipynb.\n",
      "Converted 04-datasets.ipynb.\n",
      "Converted 05-models-all.ipynb.\n",
      "Converted 05-models-losses-and-metrics-Copy1.ipynb.\n",
      "Converted 05a-models-modules.ipynb.\n",
      "Converted 05b-models-unet.ipynb.\n",
      "Converted 05c-models-losses.ipynb.\n",
      "Converted 06-callbacks.ipynb.\n",
      "Converted 06-various-tools.ipynb.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (fastai v2)",
   "language": "python",
   "name": "fastai-v2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
