{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# faimed3d layers and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# default_exp layers\n",
    "from faimed3d.basics import *\n",
    "from faimed3d.augment import *\n",
    "from faimed3d.preprocess import *\n",
    "from faimed3d.models.resnet import resnet50_3d\n",
    "from faimed3d.data import *\n",
    "from fastai.vision.learner import _default_meta, _add_norm, model_meta, create_body\n",
    "from fastai.layers import ResBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from fastai.basics import *\n",
    "from fastai.callback.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not do one pass in your dataloader, there is something wrong in it\n"
     ]
    }
   ],
   "source": [
    "d = pd.read_csv('../data/radiopaedia_cases.csv')\n",
    "dls = ImageDataLoaders3D.from_df(d, \n",
    "                                 item_tfms = ResizeCrop3D(crop_by = (0., 0.1, 0.1), resize_to = (20, 150, 150), perc_crop = True),\n",
    "                                 bs = 2, \n",
    "                                 val_bs = 2)\n",
    "\n",
    "body_3d = create_body(resnet50_3d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions\n",
    "Some functions from `fastai.layers` are needed to construct Learners (see next notebook). For this some slight modifications had to be made. \n",
    "The `in_channel` function had to be modified to also accept 3D models wich have 5D weights and the `num_features_model` function was adapted to pass a size tuple of len 3 instead of 2. The other functions were not changed but copied to avoid conflicts when loaded directly from fastai.   \n",
    "`cnn_learner_3d` is essentially the same function as fastais `cnn_learner`, just adds a new callback. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`in_channels` form fastai only returns a result if `weight.ndim == 4` but in 3D convolutional layers, it will be 5 dimensions, so the functions has to be adapted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def in_channels(m):\n",
    "    \"\"\"\n",
    "    Return the shape of the first weight layer in `m`.\n",
    "    same as fastai.vision.learner.in_channels but allows l.weight.ndim of 4 and 5\n",
    "    \"\"\"\n",
    "    for l in flatten_model(m):\n",
    "        if getattr(l, 'weight', None) is not None and l.weight.ndim in [4,5]:\n",
    "            return l.weight.shape[1]\n",
    "    raise Exception('No weight layer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`num_features_model` is unchanged, but needs to be defined here to correctly call the adapted `in_channels` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def num_features_model(m):\n",
    "    \"\"\"\n",
    "    Return the number of output features for `m`.\n",
    "    same as fastai.vision.learner.num_features_model passes model_size a len 3 tuple of sz\n",
    "\n",
    "    \"\"\"\n",
    "    sz,ch_in = 32,in_channels(m)\n",
    "    while True:\n",
    "        #Trying for a few sizes in case the model requires a big input size.\n",
    "        return model_sizes_4d(m, (sz,sz,sz))[-1][1]\n",
    "        try:\n",
    "            return model_sizes_4d(m, (sz,sz,sz))[-1][1]\n",
    "        except Exception as e:\n",
    "            sz *= 2\n",
    "            print(sz)\n",
    "            if sz > 2048: raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`model_sizes` and `dummy_eval_4d` both need to be extendet to handle multiple inputs in form if lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def model_sizes_4d(m, size=(8,64,64), n_inp=1):\n",
    "    \"Pass a dummy input through the model `m` to get the various sizes of activations. same as fastai func\"\n",
    "    with hook_outputs(m) as hooks:\n",
    "        _ = dummy_eval_4d(m, size=size, n_inp=n_inp)\n",
    "        return [o.stored[0].shape for o in hooks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def dummy_eval_4d(m, size=(8,64,64), n_inp=1):\n",
    "    \"Evaluate `m` on a dummy input of a certain `size`. Same as fastai func\"\n",
    "    ch_in = in_channels(m)\n",
    "    x = one_param(m).new(1, ch_in, *size).requires_grad_(False).uniform_(-1.,1.)\n",
    "    with torch.no_grad(): return m.eval()((x, )*n_inp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`create_cnn_model` is unchanged, but needs to be redefined to correctly call `num_features_model` which then calls the changed `in_channels` function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4D Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `faimed3d` 4D data is processed by using a unique stem for each input but a shared model body. `MulitStem` is a wrapper class, which allows to duplicate a submodule and apply every instance of the module to one instance of the input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class MultiStem(SequentialEx):\n",
    "    'applies one input of inputs to only one layer of layers'\n",
    "    def forward(self, inputs)->list:\n",
    "        out = []\n",
    "        for i, inp in enumerate(inputs):\n",
    "            out.append(self.layers[i](inp))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this, we want to send the feature maps from each stem through the same encoder. This is achieved with the `RepeatedSequential` class, which takes a list of tensors and applies it's modules to each element of the list. A list of tensors is then returned again so that multiple instances of `RepeatedSequential` can be chained after each other. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class RepeatedSequential(SequentialEx):\n",
    "    'passes multiple inputs through the same neural network'\n",
    "    def forward(self, inputs) -> list:\n",
    "        return [module(inp) for module in self for inp in inputs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main encoder for the UNet is build with `Arch4D`, which takes a encoder, splits the stem and body and convertes the stem to a `MultiStem` and the submodules of the body to `RepeatedSequential`. `Arch4D` can be indexed as the normal encoder and has the same number of subclasses. If outputs are hooked in `Arch4D` it will return a list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Arch4D(SequentialEx):\n",
    "    'repeatedly applies the same network to different inputs'\n",
    "    def __init__(self, arch, n_inp):\n",
    "        stems = MultiStem(*[arch[0] for _ in range(n_inp)]) # different stem for each input\n",
    "        body = [RepeatedSequential(l) for l in arch[1:]] # same body/shared weights for each input\n",
    "        self.layers = nn.ModuleList([stems, *body])\n",
    "\n",
    "    def forward(self, inputs)->list:\n",
    "        for l in self.layers:\n",
    "            inputs = l(inputs)\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Arch4D(body_3d, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, torch.Size([10, 2048, 2, 2, 2]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = encoder((torch.randn(10, 3, 10, 50, 50), torch.randn(10, 3, 10, 50, 50)))\n",
    "len(out), out[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([1, 128, 9, 21, 21]),\n",
       " torch.Size([1, 256, 9, 21, 21]),\n",
       " torch.Size([1, 512, 5, 11, 11]),\n",
       " torch.Size([1, 1024, 3, 6, 6]),\n",
       " torch.Size([1, 2048, 2, 3, 3])]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_sizes_4d(encoder, n_inp = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Arch4D` returns a list of tensors, which needs to be concatenated for further processing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Concat(Module):\n",
    "    def __init__(self, ni, ndim, dim = 1):\n",
    "        store_attr()\n",
    "        self.bn = BatchNorm(ni, ndim)\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self, inputs:(list, tuple))->Tensor:\n",
    "        inputs = torch.cat(inputs, self.dim)\n",
    "        return self.act(self.bn(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`fastai` performes adaptive concat pooling as first step in the new header, which is adapted to 3D. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class AdaptiveConcatPool3d(Module):\n",
    "    \"Layer that concats `AdaptiveAvgPool3d` and `AdaptiveMaxPool3d`\"\n",
    "    def __init__(self, size=None):\n",
    "        self.size = size or 1\n",
    "        self.ap = nn.AdaptiveAvgPool3d(self.size)\n",
    "        self.mp = nn.AdaptiveMaxPool3d(self.size)\n",
    "    def forward(self, x): return torch.cat([self.mp(x), self.ap(x)], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapper for 4D Models\n",
    "\n",
    "In radiology, often multiple sequences are needed for a diagnosis. For example, while viewing a head MRI DWI and ADC map have to be viewed together for a stroke diagnosis and to advoid false diagnosis from T2 shine trough. The data effectively becomes 4D. There are no modules for 4D convolution, so a workarround needs to be defined which processes the multiple 3D volumes after each other and then pools the information. The models should also still work with available pretrained 3D models, so no completly new architectures are defined in `faimed3d`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using multiple inputs, `nn.Sequential` will not work as it expects only two inputs (self and input). Therefore a subclass is defined, which accepts multiple inputs and converts those into a tuple, which is then passed to the modules. The first element of the final model will be the `MultiStem` which expects a list of tensors and returns a list of tensors. The next submodules are `RepeatedSequentials` which again take in and return a list of tensors. Next is the Concat Module, which will take in a list but return a single tensor, so the normal `fastai` head can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Sequential4D(nn.Sequential):\n",
    "    def forward(self, *input):\n",
    "        input = list(input)\n",
    "        for module in self:\n",
    "            input = module(input)\n",
    "        return input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When `fastai` creates a new model head, first the input is pooled with `AdaptiveConcatPool`, then flattened ans passed trough two linear layers might not be the best approach for multiple inputs, as the first layer would recieve `n_inp` * `n_features` inputs and reduces it to 512 features. So the first linear layer wil much more reduce the feature information than the second. So in `faimed3d` an additional convolutional layer with kernel size 1 and stride 1 is used to pool the number of features from `n_features` * `n_inp` to `n_features`. Then the normal `fastai` head is added. We added the last convolutional layer to the head, so that the freeze and unfreeze operations of `fastai` still work as expected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def create_head(nf, n_out, n_inp=None, lin_ftrs=None, ps=0.5, concat_pool=True, bn_final=False, lin_first=False, y_range=None):\n",
    "    \"Model head that takes `nf` features, runs through `lin_ftrs`, and out `n_out` classes.\"\n",
    "    lin_ftrs = [nf, 512, n_out] if lin_ftrs is None else [nf] + lin_ftrs + [n_out]\n",
    "    ps = L(ps)\n",
    "    if len(ps) == 1: ps = [ps[0]/2] * (len(lin_ftrs)-2) + ps\n",
    "    actns = [nn.ReLU(inplace=True)] * (len(lin_ftrs)-2) + [None]\n",
    "    pool = AdaptiveConcatPool3d() if concat_pool else nn.AdaptiveAvgPool3d(1)\n",
    "    if n_inp is not None:\n",
    "        pool = nn.Sequential(Concat(n_inp*nf//2, 3),\n",
    "                             # nf is not the true number of features but number of features * 2\n",
    "                             # therefore we need to divide by 2 in Concat\n",
    "                             pool,\n",
    "                             ConvLayer(n_inp*nf, nf, ndim = 3, ks = 1, stride = 1))\n",
    "    layers = [pool, Flatten()]\n",
    "    if lin_first: layers.append(nn.Dropout(ps.pop(0)))\n",
    "    for ni,no,p,actn in zip(lin_ftrs[:-1], lin_ftrs[1:], ps, actns):\n",
    "        layers += LinBnDrop(ni, no, bn=True, p=p, act=actn, lin_first=lin_first)\n",
    "    if lin_first: layers.append(nn.Linear(lin_ftrs[-2], n_out))\n",
    "    if bn_final: layers.append(nn.BatchNorm1d(lin_ftrs[-1], momentum=0.01))\n",
    "    if y_range is not None: layers.append(SigmoidRange(*y_range))\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`create_cnn_model_4d` is similar to `create_cnn_model` but expects `n_inp` as additional argument. Depending on `n_inp` the respective network is constructed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def create_cnn_model_3d(arch, n_out, n_inp, cut=None, pretrained=True, n_in=3, init=nn.init.kaiming_normal_, custom_head=None,\n",
    "                     concat_pool=True, **kwargs):\n",
    "    \"Create custom convnet architecture using `arch`, `n_in` and `n_out`. Identical to fastai func\"\n",
    "    body = create_body(arch, n_in, pretrained, cut)\n",
    "    body = Arch4D(body, n_inp)\n",
    "    if custom_head is None:\n",
    "        nf = num_features_model(body) * (2 if concat_pool else 1)\n",
    "        head = create_head(nf, n_out, n_inp, concat_pool=concat_pool, **kwargs)\n",
    "    else: head = custom_head\n",
    "    model = Sequential4D(body, head)\n",
    "    if init is not None: apply_init(model[1], init)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting model structure is shown in the below figure, using an 3D ResNet-18 as example arch.   \n",
    "![4D CNN](images/4d-cnn.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_cnn_model_3d(resnet50_3d, 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.randn(2, 3, 3, 10, 10)).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 01_basics.ipynb.\n",
      "Converted 02_preprocessing.ipynb.\n",
      "Converted 03_transforms.ipynb.\n",
      "Converted 04_dataloaders.ipynb.\n",
      "Converted 05_layers.ipynb.\n",
      "Converted 06_learner.ipynb.\n",
      "Converted 06a_models.alexnet.ipynb.\n",
      "Converted 06b_models.resnet.ipynb.\n",
      "Converted 06d_models.unet.ipynb.\n",
      "Converted 06f_models.losses.ipynb.\n",
      "Converted 07_callback.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai-latest",
   "language": "python",
   "name": "fastai-latest"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
